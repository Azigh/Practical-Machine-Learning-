---
title: "Practical Machine Learning: Final-Project"
date: "August 3rd, 2019"
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

## Project Background and Executive Summary
In this project, we will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participant They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The five ways are exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Only Class A corresponds to correct performance. Follwoing are the steps we take to lead the final results.




## Data processing
```{r}
# Loading required r-packages:

library(caret); library(rattle); library(rpart); library(rpart.plot)
library(randomForest); library(repmis)

training <- read.csv("pml-training.csv", na.strings = c("NA", ""))
testing <- read.csv("pml-testing.csv", na.strings = c("NA", ""))
```
Look at the dimensions & head of the dataset to get an idea:

```{r}
# dimension of training data
dim(training)
# dimension of testing data
dim(testing)
# Excluded because excessive amount of data
# head(training)
# Excluded because excessivness
#str(training,20)
# Excluded because excessivness
#summary(training)
```
As the dimension of trating and test sets show: The training dataset has 19622 observations and 160 variables, and the testing data set contains 20 observations and the same variables as the training set. We are trying to predict the outcome variable named "classe"" in the training set.

## Data Cleaning:

We now delete columns (predictors) of the training set that contain any missing values.
```{r}
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
```
As the data shows, we can remove the first seven predictors since these variables have little predicting power for the outcome classe.
```{r}
trainData <- training[, -c(1:7)]
testData <- testing[, -c(1:7)]
dim(trainData)
dim(testData)
```
#Preparing Data to perform Machine Learning Algorithms
In order to get out-of-sample errors we need to have a validation set(test set into training set). Therefore, we split the cleaned training set trainData into a training set (train, 70%) for prediction and a validation set (valid 30%) to compute the out-of-sample errors.

# Data Splitting

```{r}
set.seed(1226) 
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)
train <- trainData[inTrain, ]
valid <- trainData[-inTrain, ]
```

## Applying Prediction Algorithms
In this section we apply both classification trees and random forest to see the results and do the comparisions.

## First Approach: Classification Trees 
Here we apply k-fold algorithm for cross-validation. We apply K=5 (5-fold)
cross-validation.

```{r}
# Set 5-fold cross valication 
control <- trainControl(method = "cv", number = 5)

# Applying classification trees on train data set
fit_rpart <- train(classe ~ ., data = train, method = "rpart", 
                   trControl = control)
print(fit_rpart, digits = 4)
```

See how the classification tree looks like in training set:
```{r}
fancyRpartPlot(fit_rpart$finalModel)
```

See how the prediction works on validation data set. and compute the accuracy:


```{r}
# predict outcomes using validation set
predict_rpart <- predict(fit_rpart, valid)
# Show prediction result
(conf_rpart <- confusionMatrix(valid$classe, predict_rpart))
```


```{r}
(accuracy_rpart <- conf_rpart$overall[1])
```

### Result:
The results of confusion matrix shows the accuracy rate is 0.5, and so the out-of-sample error rate is 0.5. Therefore, we can see that using classification tree does not predict the outcome classe very well.

## Second Approach: Random Forest
```{r}
fit_rf <- train(classe ~ ., data = train, method = "rf",
                   trControl = control, tuneLength=1)
print(fit_rf, digits = 4)
```

See how the prediction works on validation data set. and compute the accuracy:

```{r}
# predict outcomes using validation set
predict_rf <- predict(fit_rf, valid)
# Show prediction result
(conf_rf <- confusionMatrix(valid$classe, predict_rf))
```

```{r}
(accuracy_rf <- conf_rf$overall[1])
```
For this dataset, random forest method is way better than classification tree method. The accuracy rate is 0.992.
This may be due to the fact that many predictors are highly correlated. Random forests chooses a subset of predictors at each split and decorrelate the trees.

## Prediction on Testing Set
We now use random forests to predict the outcome variable classe for the testing set.

```{r}
(predict(fit_rf, testData))
```

### Comparison of prediction results using both classification algorithms:

It is nice to see the comparison of predictions of both classification algorithms (classification trees and random forest) on the testing set:

```{r}
predictions <- t(cbind(
    randon_forest=as.data.frame(predict(fit_rf, testData), optional=TRUE),
    classification_trees=as.data.frame(predict(fit_rpart, testData), optional=TRUE)))
predictions
```





